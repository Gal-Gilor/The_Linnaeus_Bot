{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load jpgs\n",
    "def process_images(inpath, outpath, dim_tuple, extension, start=1):\n",
    "    #open images\n",
    "    for file in tqdm(glob(f'{inpath}*.{extension}')):\n",
    "        with Image.open(file) as img:\n",
    "            #rotate image\n",
    "            rotated_images = rotate_images(img)\n",
    "            \n",
    "            # resize images\n",
    "            resized = resizing(rotated_images, dim_tuple)\n",
    "            \n",
    "            #grayscale images\n",
    "            gray_images = grayscale(resized)\n",
    "            \n",
    "            \n",
    "            # save the images\n",
    "            save_preprocessed_images(gray_images, outpath, extension, start)\n",
    "            start += 3         \n",
    "             \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotate images\n",
    "def rotate_images(image):\n",
    "    '''\n",
    "    rotate_images(image):\n",
    "    This function rotates an image on it's center 7 times (45, 90, 135, 180, 225, 270, and mirror image)\n",
    "    Input:\n",
    "        One image file\n",
    "    Returns:\n",
    "        A list of images containing the original image and the rotated one\n",
    "    '''\n",
    "    rotated_images = []\n",
    "\n",
    "    rotate180 = image.rotate(180)\n",
    "    \n",
    "    chirl_image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    \n",
    "    rotated_images.extend([image, rotate180, chirl_image])  \n",
    "    return rotated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resizing(images, dim_tuple):\n",
    "    '''\n",
    "    resizing(images, dim_tuple):\n",
    "    This function resizes a list of images\n",
    "    Input:\n",
    "        List of images\n",
    "        Tuple containing the desired hight and the width\n",
    "    Returns:\n",
    "        List of resized images        \n",
    "    '''\n",
    "    resized = [image.resize(dim_tuple) for image in images]\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale(images):\n",
    "    '''\n",
    "    grayscale(images):\n",
    "    This transforms RGB images to grayscale images\n",
    "    Input:\n",
    "        List of RBG images\n",
    "    Returns:\n",
    "        List of grayscale images\n",
    "    '''\n",
    "    gray_images = [image.convert(mode='L') for image in images]\n",
    "    return gray_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save images in a different path\n",
    "def save_preprocessed_images(processed_images, outpath, extension, start):\n",
    "    '''\n",
    "    save_preprocessed_images(processed_images, outpath, extension, start)\n",
    "    This function saves any type of image in a specific directory\n",
    "    Input:\n",
    "        processed_images: List of images\n",
    "        outpath: Where you want the files to be saved\n",
    "    Returns:\n",
    "        Does not return a variable. Creates image files \n",
    "    '''\n",
    "    [image.save(f'{outpath}\\\\image{i}.{extension}') for i, image in enumerate(processed_images, start)]\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_train_test(inpath, train_path, test_path, extension, n, label):\n",
    "    '''\n",
    "    pickle_train_test(inpath, train_path, test_path, extension, n, label):\n",
    "    This function creates two datasets from the contents of a specific directory\n",
    "    Input:\n",
    "        inpath: The path your files are at\n",
    "        train_path: The path you want to save your train set\n",
    "        test_path: The path you want to save your test set\n",
    "        n: Train set size\n",
    "        label: Label (int)\n",
    "            \n",
    "    '''\n",
    "    images = []\n",
    "    #open images\n",
    "    for file in tqdm(glob(f'{inpath}*.{extension}')):\n",
    "        with Image.open(file) as img:\n",
    "            np_image = np.asarray(img) / 255\n",
    "            images.append([np_image, label])\n",
    "            if len(images) == n:\n",
    "                with open(train_path, 'wb') as file:\n",
    "                    pickle.dump(images, file)\n",
    "                del images\n",
    "                images = []\n",
    "            \n",
    "    with open(test_path, 'wb') as file:\n",
    "        pickle.dump(images, file)\n",
    "    del images\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Damselflies Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and save new images\n",
    "inpath = r'E:\\images2019\\train_val2019\\Damselflies\\\\'\n",
    "extension = 'jpg'\n",
    "outpath = r'E:\\processed_damsel_images'\n",
    "dim_tuple = (256, 256)\n",
    "\n",
    "# process_images(inpath, outpath, dim_tuple, extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Dragonflies Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpath = r'E:\\images2019\\train_val2019\\Dragonflies\\\\'\n",
    "extension = 'jpg'\n",
    "outpath = r'E:\\processed_dragon_images'\n",
    "dim_tuple = (256, 256)\n",
    "\n",
    "# process_images(inpath, outpath, dim_tuple, extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Dragonfly Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inpath = r'E:\\processed_dragon_images\\\\'\n",
    "test_path = r'E:\\processed_dragon_images\\test\\dragon_test.pkl'\n",
    "train_path = r'E:\\processed_dragon_images\\train\\dragon_train.pkl'\n",
    "extension = 'jpg'\n",
    "\n",
    "files = os.listdir(inpath) # dir is your directory path\n",
    "n_files = len(files)\n",
    "n_train = int(n_files * 0.8)\n",
    "print(f'{n_files}, {n_train}')\n",
    "\n",
    "# pickle_train_test(inpath, train_path, test_path, extension, n_train, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Damselfly Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpath = r'E:\\processed_damsel_images\\\\'\n",
    "test_path = r'E:\\processed_damsel_images\\test\\damsel_test.pkl'\n",
    "train_path = r'E:\\processed_damsel_images\\train\\damsel_train.pkl'\n",
    "extension = 'jpg'\n",
    "\n",
    "files = os.listdir(inpath) # dir is your directory path\n",
    "n_files = len(files)\n",
    "n_train = int(n_files * 0.8)\n",
    "print(f'{n_files}, {n_train}')\n",
    "\n",
    "# pickle_train_test(inpath, train_path, test_path, extension, n_train, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r'E:\\processed_damsel_images\\train\\damsel_train.pkl'\n",
    "with open(train_path, \"rb\") as train_file:\n",
    "    damsel_train = pickle.load(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[array([[0.34509804, 0.35294118, 0.39607843, ..., 0.33333333, 0.35294118,\n",
      "        0.35294118],\n",
      "       [0.33333333, 0.34117647, 0.38431373, ..., 0.3372549 , 0.35686275,\n",
      "        0.35294118],\n",
      "       [0.32941176, 0.3372549 , 0.38039216, ..., 0.34901961, 0.36078431,\n",
      "        0.35686275],\n",
      "       ...,\n",
      "       [0.46666667, 0.45490196, 0.44313725, ..., 0.6627451 , 0.71764706,\n",
      "        0.74509804],\n",
      "       [0.4627451 , 0.45490196, 0.45098039, ..., 0.64705882, 0.67843137,\n",
      "        0.68627451],\n",
      "       [0.4627451 , 0.45490196, 0.45490196, ..., 0.64313725, 0.65882353,\n",
      "        0.64705882]]), 1]\n",
      "<class 'list'>\n",
      "20310\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(type(damsel_train))\n",
    "print(damsel_train[0])\n",
    "print(type(damsel_train[0]))\n",
    "print(len(damsel_train))\n",
    "print(len(damsel_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dragon = [np.array(cv2.imread(file)) for file in tqdm(glob(r'E:\\processed_dragon_images\\*.jpg'))]\n",
    "print('Got dragonflies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image = plt.imshow(damsel_train[0][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(r'E:\\processed_damsel_images\\image1.jpg')\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#== Parameters =======================================================================\n",
    "BLUR = 21\n",
    "CANNY_THRESH_1 = 10\n",
    "CANNY_THRESH_2 = 300\n",
    "MASK_DILATE_ITER = 5\n",
    "MASK_ERODE_ITER = 5\n",
    "MASK_COLOR = (0.0,0.0,1.0) # In BGR format\n",
    "\n",
    "\n",
    "#== Processing =======================================================================\n",
    "\n",
    "#-- Read image -----------------------------------------------------------------------\n",
    "img = cv2.imread(r'E:\\processed_damsel_images\\image1.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#-- Edge detection -------------------------------------------------------------------\n",
    "edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)\n",
    "edges = cv2.dilate(edges, None)\n",
    "edges = cv2.erode(edges, None)\n",
    "\n",
    "#-- Find contours in edges, sort by area ---------------------------------------------\n",
    "contour_info = []\n",
    "# _, contours, B = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "# Previously, for a previous version of cv2, this line was: \n",
    "contours, _ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "# Thanks to notes from commenters, I've updated the code but left this note\n",
    "for c in contours:\n",
    "    contour_info.append((c,\n",
    "                         cv2.isContourConvex(c),\n",
    "                         cv2.contourArea(c)\n",
    "                       ))\n",
    "contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)\n",
    "max_contour = contour_info[0]\n",
    "\n",
    "#-- Create empty mask, draw filled polygon on it corresponding to largest contour ----\n",
    "# Mask is black, polygon is white\n",
    "mask = np.zeros(edges.shape)\n",
    "cv2.fillConvexPoly(mask, max_contour[0], (255))\n",
    "\n",
    "#-- Smooth mask, then blur it --------------------------------------------------------\n",
    "mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)\n",
    "mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)\n",
    "mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)\n",
    "mask_stack = np.dstack([mask]*3)    # Create 3-channel alpha mask\n",
    "\n",
    "#-- Blend masked img into MASK_COLOR background --------------------------------------\n",
    "mask_stack  = mask_stack.astype('float32') / 255.0          # Use float matrices, \n",
    "img         = img.astype('float32') / 255.0                 #  for easy blending\n",
    "\n",
    "masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR) # Blend\n",
    "masked = (masked * 255).astype('uint8')                     # Convert back to 8-bit \n",
    "\n",
    "# split image into channels\n",
    "c_red, c_green, c_blue = cv2.split(img)\n",
    "\n",
    "# merge with mask got on one of a previous steps\n",
    "img_a = cv2.merge((c_red, c_green, c_blue, mask.astype('float32') / 255.0))\n",
    "\n",
    "# show on screen (optional in jupiter)\n",
    "\n",
    "plt.imshow(img_a)\n",
    "plt.show()\n",
    "\n",
    "# cv2.imshow('img', masked)                                   # Display\n",
    "# cv2.waitKey()\n",
    "\n",
    "#cv2.imwrite('C:/Temp/person-masked.jpg', masked)           # Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from sklearn.cluster import KMeans\n",
    "from keras import callbacks\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, Input\n",
    "from keras.initializers import VarianceScaling\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from tensorflow.keras import backend as k\n",
    "from scipy.misc import imread\n",
    "from sklearn.metrics import accuracy_score, normalized_mutual_info_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(damsel_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.34509804, 0.35294118, 0.39607843, ..., 0.33333333, 0.35294118,\n",
       "        0.35294118],\n",
       "       [0.80392157, 0.82352941, 0.82745098, ..., 0.6745098 , 0.71764706,\n",
       "        0.6627451 ],\n",
       "       [0.54509804, 0.5254902 , 0.51764706, ..., 0.79215686, 0.78823529,\n",
       "        0.78823529],\n",
       "       ...,\n",
       "       [0.48235294, 0.43921569, 0.4       , ..., 0.22352941, 0.21568627,\n",
       "        0.21176471],\n",
       "       [0.43137255, 0.43137255, 0.42352941, ..., 0.34117647, 0.3372549 ,\n",
       "        0.3372549 ],\n",
       "       [0.42352941, 0.42352941, 0.42352941, ..., 0.39215686, 0.4       ,\n",
       "        0.40392157]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "damsel_tensors[:17000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "damsel_tensors = [np.array(damsel[0][0]) for damsel in damsel_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "damsel_tensors = np.asarray(damsel_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 4352000 into shape (65536)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-3fd5ff728ba3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdamsel_tensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m17000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m65536\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdamsel_tensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m17000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m65536\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# this is our input placeholder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0minput_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m65536\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 4352000 into shape (65536)"
     ]
    }
   ],
   "source": [
    "x_train = damsel_tensors[:17000].reshape(-1, 65536)\n",
    "x_test = damsel_tensors[17000:].reshape(-1, 65536)\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(65536,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(4000, activation='tanh')(input_img)\n",
    "encoded = Dense(2000, activation='relu')(encoded)\n",
    "encoded = Dense(1000, activation='relu')(encoded)\n",
    "encoded = Dense(500, activation='relu')(encoded)\n",
    "encoded = Dense(10, activation='sigmoid')(encoded)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(500, activation='relu')(encoded)\n",
    "decoded = Dense(1000, activation='relu')(decoded)\n",
    "decoded = Dense(2000, activation='relu')(decoded)\n",
    "decoded = Dense(4000, activation='relu')(decoded)\n",
    "decoded = Dense(1)(decoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 18000 arrays: [array([[0.34509804, 0.35294118, 0.39607843, ..., 0.64313725, 0.65882353,\n        0.64705882]]), array([[0.80392157, 0.82352941, 0.82745098, ..., 0.5254902 , 0.49803922,\n        0.49411765]]), array([...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-250563e5f54f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m train_history = autoencoder.fit(x_train, x_train, epochs=1, batch_size=1, \n\u001b[1;32m----> 9\u001b[1;33m                                 validation_data=(x_test, x_test), callbacks=[estop])\n\u001b[0m",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[1;34m'Expected to see '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' array(s), '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[1;34m'but instead got the following list of '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 18000 arrays: [array([[0.34509804, 0.35294118, 0.39607843, ..., 0.64313725, 0.65882353,\n        0.64705882]]), array([[0.80392157, 0.82352941, 0.82745098, ..., 0.5254902 , 0.49803922,\n        0.49411765]]), array([..."
     ]
    }
   ],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "autoencoder.compile(optimizer='SGD', loss='mse')\n",
    "\n",
    "estop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "train_history = autoencoder.fit(x_train, x_train, epochs=1, batch_size=1, \n",
    "                                validation_data=(x_test, x_test), callbacks=[estop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip --upgrade tensorflow==2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
